{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53b3f2db-891c-4bf7-9eb3-01ff5e001d05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# `What is Dataframe` :\n",
    "\n",
    "  - A DataFrame is a distributed collection of data organized into columns and rows(similar to a table). \n",
    "\n",
    "###  `Properties of DataFrame`:\n",
    "  1. **Distributed:** Data is distributed across multiple nodes in a cluster.\n",
    "  2. **Immutable:** Once created, it cannot be changed. Transformations produce new DataFrames.\n",
    "  3. **Lazy Evaluation:** Transformations on DataFrames are not computed immediately. Spark computes them only when an action requires a result to be returned to the driver program.\n",
    "  4. **Schema:** Each DataFrame has a schema, representing the structure of the data, including column names and types.\n",
    "  5. **Supports various data formats:** Can read and write data in various formats like CSV, JSON, Parquet, Avro, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58a5c5a0-5b2e-44a6-9776-132f6ff50e37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create DF from Reading Multiple File Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de1eb118-7334-4a65-899b-97a7d0fbb021",
     "showTitle": true,
     "title": "CSV"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "# define the custom schema using structtype and structfield\n",
    "books_shema = StructType([\n",
    "    StructField(\"book_id\",StringType()),\n",
    "    StructField(\"title\",StringType()),\n",
    "    StructField(\"author\",StringType()),\n",
    "    StructField(\"category\",StringType()),\n",
    "    StructField(\"price\", IntegerType())\n",
    "])\n",
    "\n",
    "# df_csv = spark.read.csv(\"dbfs:/mnt/adls_container/books-data.csv\", header=True, sep=\";\", inferSchema=True)\n",
    "df_csv = spark.read.csv(\"dbfs:/mnt/adls_container/books-data.csv\", header=True, sep=\";\",schema=books_shema)\n",
    "\n",
    "display(df_csv)\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4106d30-f0b5-4153-a214-3171ed605713",
     "showTitle": true,
     "title": "JSON"
    }
   },
   "outputs": [],
   "source": [
    "df_json = spark.read.json(\"dbfs:/mnt/adls_container/customers-data.json\")\n",
    "\n",
    "display(df_json.limit(10))\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fffe56e-77b2-4bf6-b146-05e54c42461f",
     "showTitle": true,
     "title": "PARQUET"
    }
   },
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(\"dbfs:/FileStore/parquet/\")\n",
    "\n",
    "display(df_parquet.limit(10))\n",
    "df_parquet.printSchema()\n",
    "df_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb6cb59b-e40f-4fe4-a218-403894d313b3",
     "showTitle": true,
     "title": "DB TABLE"
    }
   },
   "outputs": [],
   "source": [
    "driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "url = \"jdbc:sqlserver://azure-sql-server-1111.database.windows.net:1433;database=azure-sql-db-111\"\n",
    "username = \"myuser\"\n",
    "password = \"mypass@123\"  \n",
    "table = \"[SalesLT].[Customer]\"\n",
    "\n",
    "\n",
    "customer_df = (spark.read\n",
    "                   .format(\"jdbc\")\n",
    "                   .option(\"driver\", driver)\n",
    "                   .option(\"url\", url)\n",
    "                   .option(\"dbtable\", table)\n",
    "                   .option(\"user\", username)\n",
    "                   .option(\"password\", password)\n",
    "                   .load())\n",
    "\n",
    "display(customer_df.limit(10))\n",
    "customer_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c3dfcae-18a6-42a7-b9d5-7c19517bab98",
     "showTitle": true,
     "title": "hardCoded data"
    }
   },
   "outputs": [],
   "source": [
    "emp_data = [\n",
    "    (1, \"John Doe\", \"Male\", 60000.0, \"USA\"),\n",
    "    (2, \"Jane Smith\", \"Female\", 55000.0, \"Canada\"),\n",
    "    (3, \"Alice Johnson\", \"Female\", 65000.0, \"UK\"),\n",
    "    (4, \"Bob Williams\", \"Male\", 62000.0, \"Australia\"),\n",
    "    (5, \"Eve Davis\", \"Female\", 70000.0, \"India\"),\n",
    "    (5, \"Eve Davis\", \"Female\", 70000.0, \"India\"),\n",
    "    (6, \"Charlie Brown\", \"Male\", 58000.0, \"Germany\"),\n",
    "    (7, \"Diana Miller\", \"Female\", 60000.0, \"France\"),\n",
    "    (8, \"Frank Johnson\", \"Male\", 62000.0, \"Spain\"),\n",
    "    (9, \"Grace Wilson\", \"Female\", 54000.0, \"Italy\"),\n",
    "    (10, \"Henry Davis\", \"Male\", 68000.0, \"Japan\"),\n",
    "    (9, \"Grace Wilson\", \"Female\", 54000.0, \"Italy\"),\n",
    "    (10, \"Henry Davis\", \"Male\", 68000.0, \"Japan\"),\n",
    "    (11, \"Isabel Clark\", \"Female\", 59000.0, \"Brazil\"),\n",
    "    (12, \"Jack Turner\", \"Male\", 63000.0, \"Mexico\"),\n",
    "    (13, \"Katherine White\", \"Female\", 67000.0, \"South Africa\"),\n",
    "    (14, \"Louis Harris\", \"Male\", 56000.0, \"Russia\"),\n",
    "    (15, \"Mia Lee\", \"Female\", 61000.0, \"China\"),\n",
    "    (14, \"Louis Harris\", \"Male\", 56000.0, \"Russia\"),\n",
    "    (15, \"Mia Lee\", \"Female\", 61000.0, \"China\")\n",
    "]\n",
    "\n",
    "emp_schema = StructType([\n",
    "    StructField(\"empId\", IntegerType(), True),\n",
    "    StructField(\"empName\", StringType(), True),\n",
    "    StructField(\"empGender\", StringType(), True),\n",
    "    StructField(\"empSalary\", FloatType(), True),\n",
    "    StructField(\"empCountry\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_sample = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "\n",
    "df_sample.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65ae06ad-cdf5-48a0-8b31-4a782a057005",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Transformations on Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb87c396-451f-4fea-a8a7-ae27b694be98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add two more columns\n",
    "    # origin ==> constant column ==> \"India\"\n",
    "    # tax ==> derived column ==> \"12%(salary)\"\n",
    "\n",
    "# withColumn() : to add columns\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df2 = df_sample.withColumn(\"origin\", lit(\"india\")) \\\n",
    "               .withColumn(\"tax\", df_sample.empSalary * 0.12)\n",
    "\n",
    "display(df2.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e240256b-9f82-450c-a63b-c2cc0d6453c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# withColumnRenamed() : to rename the columns\n",
    "    # two columns => country, tax\n",
    "\n",
    "df3 = df2.withColumnRenamed(\"origin\", \"empOrigin\") \\\n",
    "         .withColumnRenamed(\"tax\", \"empTax\")\n",
    "\n",
    "display(df3.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0bd47a-eba0-47bb-b57a-fc58ce7a8478",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df3.select(\"empId\", df3.empName, col(\"empGender\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d60b9ee-af01-4f51-8a8c-99f17ade7551",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5951ce9-41c8-43dd-8ee5-38dd549acae2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# case conditions : when(), otherwise()\n",
    "    # male ==> m\n",
    "    # female ==> f\n",
    "    # unknown ==> u\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df4 = df3.select(\n",
    "    \"empId\",\n",
    "    \"empName\",\n",
    "    when(df3.empGender == \"Male\", \"m\").when(df3.empGender == \"Female\", \"f\").otherwise(\"u\").alias(\"empGender\"),\n",
    "    \"empSalary\",\n",
    "    \"empCountry\", \n",
    "    \"empOrigin\", \n",
    "    \"empTax\"\n",
    "    )\n",
    "\n",
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d645e938-6ce2-4811-a333-0315534492c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# orderBy() or sort() : to sort the data based on columns\n",
    "\n",
    "df4.sort(df4.empSalary.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd73a0b-70cb-4235-a35e-e40aa1b37d48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dropDuplicates() : to remove the duplicates\n",
    "\n",
    "df4.dropDuplicates().orderBy(df4.empSalary.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2607ed0-ba0f-4512-80f0-20d5c49a9912",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## where() or filter() : to filter the data\n",
    "\n",
    "(df4\n",
    "    .dropDuplicates()\n",
    "    .filter((df4.empSalary > 55000) & (df4.empGender == 'f') & (df4.empName.like(\"%e\")))\n",
    "    .sort(df4.empSalary.desc())\n",
    "    .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23344202-2703-4e54-9294-002f9478df9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[(1, 'anil', 'M', 5000, 'IT'),\\\n",
    "      (2, 'sandeep', 'M',6000, 'IT'),\\\n",
    "      (3, 'riya', 'F',2500, 'payroll'),\\\n",
    "      (4, 'prteek', 'M',4000, 'HR'),\\\n",
    "      (5, 'vani', 'F',2000, 'HR'),\\\n",
    "      (6, 'sunil', 'M', 2000, 'payroll'),\\\n",
    "      (7, 'diksha', 'F',3000, 'IT'),\n",
    "      (8, 'rajesh', 'M', 4500, 'Finance'),\n",
    "      (9, 'neha', 'F', 3500, 'Finance'),\n",
    "      (10, 'amit', 'M', 3000, 'HR'),\n",
    "      (11, 'pooja', 'F', 5500, 'IT'),\n",
    "      (12, 'rohit', 'M', 6000, 'IT')\n",
    "      ]\n",
    "\n",
    "# Define the schema for the data\n",
    "schema = StructType([\n",
    "    StructField(\"empId\", IntegerType(), True),\n",
    "    StructField(\"empName\", StringType(), True),\n",
    "    StructField(\"empGender\", StringType(), True),\n",
    "    StructField(\"empSalary\", IntegerType(), True),\n",
    "    StructField(\"empDepartment\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b59cc1-b8f7-4851-aca2-475bf789adbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# aggregate functions : count, max, min, sum, avg\n",
    "\n",
    "from pyspark.sql.functions import count, max, min, sum, avg\n",
    "\n",
    "df.agg(count(\"*\").alias(\"totalEmpCount\")).show()\n",
    "df.agg(max(\"empSalary\").alias(\"maxSalary\")).show()\n",
    "df.agg(min(\"empSalary\").alias(\"minSalary\")).show()\n",
    "df.agg(avg(\"empSalary\").alias(\"avgSalary\")).show()\n",
    "df.agg(sum(\"empSalary\").alias(\"sumSalary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d70a75be-330e-464a-a0fa-c8906395562a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.agg(\n",
    "    count(\"*\").alias(\"totalEmpCount\"),\n",
    "    max(\"empSalary\").alias(\"maxSalary\"),\n",
    "    min(\"empSalary\").alias(\"minSalary\"),\n",
    "    avg(\"empSalary\").alias(\"avgSalary\"),\n",
    "    sum(\"empSalary\").alias(\"sumSalary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d892f715-c46f-4757-978d-ba002888b630",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# groupBy() : to group the data\n",
    "\n",
    "df.groupBy(\"empDepartment\").agg(\n",
    "                                count(\"*\").alias(\"totalEmpCount\"),\n",
    "                                max(\"empSalary\").alias(\"maxSalary\"),\n",
    "                                min(\"empSalary\").alias(\"minSalary\"),\n",
    "                                avg(\"empSalary\").alias(\"avgSalary\"),\n",
    "                                sum(\"empSalary\").alias(\"sumSalary\")\n",
    "                            ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185e50b3-8567-4d3e-a01e-8a9dfce330c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# union / unionAll : to merge the data vertically (both works same way, allows duplicates)\n",
    "\n",
    "data1 = [(1, 'Anil',27), \n",
    "         (2, 'sandeep', 28), \n",
    "         (3, 'riya', 29)]  #jan\n",
    "schema1 = ['id', 'name', 'age']\n",
    "\n",
    "data2 = [(3, 'riya', 29), \n",
    "         (4, 'rani', 26)] #feb\n",
    "schema2 = ['id', 'name', 'age']\n",
    "\n",
    "data3 = [(5, 'liya', 29), \n",
    "         (6, 'mani', 26)] #march\n",
    "schema3 = ['id', 'name', 'age']\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "df3 = spark.createDataFrame(data3, schema3)\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "173903a4-4108-4ab1-96fe-6a4b2fe48fac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_union = df1.union(df2).union(df3)\n",
    "\n",
    "df_union.dropDuplicates().sort(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32e8ec2-87b8-4507-b06c-2ad7616dacbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_data = [\n",
    "    (1, \"John\", 1, 50000, 1),\n",
    "    (2, \"Alice\", 2, 60000, 2),\n",
    "    (3, \"Bob\", 3, 55000, 2),\n",
    "    (2, \"Alice\", 2, 60000, 2),\n",
    "    (3, \"Bob\", 3, 55000, 2),\n",
    "    (4, \"Jane\", 4, 52000, 3),\n",
    "    (5, \"Eve\", None, 48000, 4),\n",
    "    (6, \"Charlie\", 4, 47000, None),\n",
    "    (7, \"David\", 2, 55000, 3),\n",
    "    (8, \"Linda\", 3, 53000, 1),\n",
    "    (9, \"Frank\", None, 59000, 4),\n",
    "    (7, \"David\", 2, 55000, 3),\n",
    "    (8, \"Linda\", 3, 53000, None),\n",
    "    (9, \"Frank\", 1, 59000, 4),\n",
    "    (10, \"Grace\",1, 49000, None),\n",
    "    (10, \"Grace\",1, 49000, 4)]\n",
    "emp_schema = [\"empId\", \"empName\", \"deptId\", \"empSalary\", \"cityId\"]\n",
    "\n",
    "dept_data = [\n",
    "    (1, \"HR\"),\n",
    "    (2, \"IT\"),\n",
    "    (3, \"Sales\"),\n",
    "    (4, \"Finance\"),\n",
    "]\n",
    "dept_schema = [\"deptId\", \"deptName\"]\n",
    "\n",
    "address_data = [\n",
    "    (1, \"hyd\"),\n",
    "    (2, \"blr\"),\n",
    "    (3, \"chn\"),\n",
    "    (4, \"kkt\")\n",
    "]\n",
    "add_schema = [\"cityId\", \"cityName\"]\n",
    "\n",
    "print(\"emp_df :\")\n",
    "emp_df = spark.createDataFrame(emp_data,emp_schema)\n",
    "emp_df.show()\n",
    "\n",
    "print(\"dept_df :\")\n",
    "dept_df = spark.createDataFrame(dept_data,dept_schema)\n",
    "dept_df.show()\n",
    "\n",
    "print(\"address_df :\")\n",
    "address_df = spark.createDataFrame(address_data,add_schema)\n",
    "address_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc9d710-c8a5-430a-9e39-3adefd5d3c44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## joins types : inner, left, right, full, cross, self\n",
    "\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "df_inner = (emp_df.join(dept_df, emp_df.deptId == dept_df.deptId, \"inner\")\n",
    "                  .join(address_df, emp_df.cityId == address_df.cityId, \"inner\")\n",
    "                  .drop(emp_df.cityId, emp_df.deptId)\n",
    "                  .dropDuplicates()\n",
    "                  .sort(\"empId\")\n",
    "                  .filter(col(\"CityName\") == \"chn\")\n",
    "                  .withColumn(\"createdAt\", current_timestamp())\n",
    ")\n",
    "\n",
    "\n",
    "df_inner.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b759bef-f77e-408c-a302-f3b7d32e7baa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_full = (emp_df.join(dept_df, emp_df.deptId == dept_df.deptId, \"full\")\n",
    "                  .join(address_df, emp_df.cityId == address_df.cityId, \"full\")\n",
    "                  .drop(emp_df.cityId, emp_df.deptId)\n",
    "                  .dropDuplicates([\"empId\"])\n",
    "                  .sort(\"empId\")\n",
    "                  .filter(col(\"CityName\") == \"chn\")\n",
    "                  .withColumn(\"createdAt\", current_timestamp())\n",
    ")\n",
    "\n",
    "df_full.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c681129d-e716-44e2-a9bd-0b60e6d946dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## ranking functions : row_number, rank, dense_rank\n",
    "\n",
    "salary     row_number      rank(skip)      dense_rank(no_skipping)\n",
    "100         1               1                 1\n",
    "100         2               1                 1\n",
    "200         3               3                 2\n",
    "300         4               4                 3\n",
    "300         5               4                 3\n",
    "300         6               4                 3\n",
    "400         7               7                 4\n",
    "500         8               8                 5\n",
    "500         9               8                 5\n",
    "500         10              8                 5\n",
    "500         11              8                 5\n",
    "600         12              12                6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a52726f-a3e7-428f-8e1e-f9c4b40f1383",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"John\", 50000, \"IT\"),\n",
    "    (\"Alice\", 60000, \"HR\"),\n",
    "    (\"Bob\", 55000, \"IT\"),\n",
    "    (\"Jane\", 52000, \"Finance\"),\n",
    "    (\"Eve\", 48000, \"HR\"),\n",
    "    (\"lavanya\", 55000, \"IT\"),\n",
    "    (\"ALEX\", 60000, \"HR\"),\n",
    "    (\"Charlie\", 47000, \"Finance\"),\n",
    "    (\"David\", 55000, \"IT\"),\n",
    "    (\"Linda\", 53000, \"Finance\"),\n",
    "    (\"Frank\", 59000, \"HR\"),\n",
    "    (\"Grace\", 49000, \"IT\"),\n",
    "    (\"Raghu\", 53000, \"Finance\"),\n",
    "]\n",
    "schema = [\"empName\", \"salary\", \"department\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a6c543-e1a6-4952-a070-a5e969b48a21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, col\n",
    "\n",
    "window_spec = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df1 = (df.withColumn(\"row_numColumn\", row_number().over(window_spec))\n",
    "         .withColumn(\"rankColumn\", rank().over(window_spec))\n",
    "         .withColumn(\"dense_rankColumn\", dense_rank().over(window_spec))\n",
    ")\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d8d5c2-81c4-4bd2-8fb0-b098393a75bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# second highest salary \n",
    "\n",
    "df1.filter(df1.dense_rankColumn == 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a0dd6e-b97b-4d90-a5ee-a73fb9fabd9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, col\n",
    "\n",
    "window_spec = Window.orderBy(col(\"salary\").desc()).partitionBy(\"department\")\n",
    "\n",
    "df1 = (df.withColumn(\"row_numColumn\", row_number().over(window_spec))\n",
    "         .withColumn(\"rankColumn\", rank().over(window_spec))\n",
    "         .withColumn(\"dense_rankColumn\", dense_rank().over(window_spec))\n",
    ")\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7773945f-4c99-4718-859b-f12fad4fcc53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# second highest salary for each department\n",
    "\n",
    "df1.filter(df1.dense_rankColumn == 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e3ca15a-57cf-4ed1-9325-ce6772bc6954",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", 29, \"New York\"),\n",
    "    (2, \"Bob\", None, \"Los Angeles\"),\n",
    "    (3, None, 23, None),\n",
    "    (4, \"David\", None, \"San Francisco\"),\n",
    "    (5, \"Eve\", 35, None),\n",
    "    (6, None, None, None)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c88b757f-f7a7-4354-8750-fe876f655db7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop the records with null values in any column\n",
    "\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b93367b0-88d5-49f6-bec3-aa9241e672f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop the records with null values in specific columns\n",
    "\n",
    "df.na.drop(subset=[\"name\", \"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95ad90c3-0fe5-428e-9dfa-c7279ab2c992",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drops those records where a threshold number of columns are null\n",
    "\n",
    "df.na.drop(thresh=3).show() # 3 non-null columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6efcef-b5e5-4194-9b86-455afa880e1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filling nulls in all columns\n",
    "\n",
    "df.na.fill(\"unknown\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6843a01-a827-43c9-af0e-8fd33b20b006",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fill in specific columns \n",
    "\n",
    "df.na.fill(\"unknown\", subset=[\"age\", \"city\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfcdf109-0e83-4ec0-9f87-e5954f1ab01c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter rows with nulls or not nulls\n",
    "\n",
    "df.filter(col(\"age\").isNull()).show()\n",
    "\n",
    "df.filter(col(\"age\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4803c5f-2ab5-472a-b620-6253c0aa8187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# coalese nulls : returns first non-null value\n",
    "\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "df1 = df.withColumn(\"nw_col\", coalesce(col(\"age\"), col(\"id\")))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f286220-3310-44d4-b48c-42dcccff07b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"John,Doe,25,Engineer\",),\n",
    "        (\"Jane,Smith,30,Data Scientist\",),\n",
    "        (\"Bob,Johnson,22,Analyst\",),\n",
    "        (\"Alice,Williams,28,Manager\",),\n",
    "        (\"Charlie,Brown,35,Developer\",)]\n",
    "\n",
    "columns = [\"full_details\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d906bac-8629-4fb2-b1bb-48d6f045f460",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## split() : to split the data based on delimeter \n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df1 = (df.withColumn(\"firstname\", split(df.full_details, \",\")[0])\n",
    "         .withColumn(\"lastName\", split(df.full_details, \",\")[1])\n",
    "         .withColumn(\"Age\", split(df.full_details, \",\")[2])\n",
    "         .withColumn(\"role\", split(df.full_details, \",\")[3])\n",
    "         .drop(df.full_details))\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4cb4aeb-e296-429b-acf9-7ef9b38ad822",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# concat() : to concatenate the columns\n",
    "\n",
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "df2 = df1.withColumn(\"email\", concat(df1.firstname,lit(\".\"), df1.lastName, lit(\"@gmail.com\")))\n",
    "\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b5c238a-bcfa-4b88-841a-2b9cfa840ce6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# assignement : to extract a domain from email address (charindex, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a21243d3-5709-4ab6-982b-a61f642f3dc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# substring : to extract part of a string\n",
    "\n",
    "from pyspark.sql.functions import substring, substring_index\n",
    "\n",
    "df3 = df2.withColumn(\"domain\", substring_index(df2.email, \"@\", -1)) \\\n",
    "         .withColumn(\"new_role\",substring(\"role\", 1, 3))\n",
    "\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44450a0e-0e2c-413d-aa07-19ecff888e71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.3 - Dataframes and Operations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
